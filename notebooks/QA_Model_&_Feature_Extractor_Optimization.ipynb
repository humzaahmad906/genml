{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "QA_Model_&_Feature_Extractor_Optimization.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "a0ef06c15b14496da01b239592f10bef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4c06e8ce9624b32b56797d7f72fa68d",
       "IPY_MODEL_cdfc81d124354bf1a329f22c595852d9",
       "IPY_MODEL_d9c757df508944daabb0d13d50e1496b"
      ],
      "layout": "IPY_MODEL_5959295bcf9843d9b1a1203bb071f02c"
     }
    },
    "d4c06e8ce9624b32b56797d7f72fa68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d2a32dc0ef64f8b9d362978917f31b8",
      "placeholder": "​",
      "style": "IPY_MODEL_2fc20f5e83154a2b9f0c4dd7d83140c4",
      "value": "Downloading: 100%"
     }
    },
    "cdfc81d124354bf1a329f22c595852d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41a11af023264d809af04468e11a3e35",
      "max": 498,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04ec75fe4aca42a3b70bfa34203785fe",
      "value": 498
     }
    },
    "d9c757df508944daabb0d13d50e1496b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e464fb4ba31849c6a8156dc28a95b711",
      "placeholder": "​",
      "style": "IPY_MODEL_00820d3385294adfbfc1d891be3b0ca8",
      "value": " 498/498 [00:00&lt;00:00, 7.87kB/s]"
     }
    },
    "5959295bcf9843d9b1a1203bb071f02c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d2a32dc0ef64f8b9d362978917f31b8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fc20f5e83154a2b9f0c4dd7d83140c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41a11af023264d809af04468e11a3e35": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04ec75fe4aca42a3b70bfa34203785fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e464fb4ba31849c6a8156dc28a95b711": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00820d3385294adfbfc1d891be3b0ca8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94ccc999334044cdbd502d5c5cf021bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18ba8f1c9abd448b99935ce008c00c23",
       "IPY_MODEL_78b99f15dbce47cb9aaf92c6b891b4d7",
       "IPY_MODEL_c8d23799f62b4cb2915c94b86332bf68"
      ],
      "layout": "IPY_MODEL_a9a885bc5b7e49778fc19675ed71cf58"
     }
    },
    "18ba8f1c9abd448b99935ce008c00c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c10a910ceb074e24aff32369f64f5155",
      "placeholder": "​",
      "style": "IPY_MODEL_9a8544fb1e124d0f8a66cb98cc174a15",
      "value": "Downloading: 100%"
     }
    },
    "78b99f15dbce47cb9aaf92c6b891b4d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f2dad47f9da481cb185feab60ab4e06",
      "max": 571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51fdd1c8de3f491ea8f86cc86665ff5d",
      "value": 571
     }
    },
    "c8d23799f62b4cb2915c94b86332bf68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4854182869074745873f5d7f4a0d961c",
      "placeholder": "​",
      "style": "IPY_MODEL_8cde3c6bd0f44f6bbc1ec247925af7e2",
      "value": " 571/571 [00:00&lt;00:00, 11.6kB/s]"
     }
    },
    "a9a885bc5b7e49778fc19675ed71cf58": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c10a910ceb074e24aff32369f64f5155": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a8544fb1e124d0f8a66cb98cc174a15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f2dad47f9da481cb185feab60ab4e06": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51fdd1c8de3f491ea8f86cc86665ff5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4854182869074745873f5d7f4a0d961c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cde3c6bd0f44f6bbc1ec247925af7e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "us2326B97g0w",
    "outputId": "80927cf3-8cfe-4805-9b50-723c41771fdc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Using cached transformers-4.21.1-py3-none-any.whl (4.7 MB)\r\n",
      "Requirement already satisfied: requests in /home/humza/miniconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\r\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 6.6 MB 2.2 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting regex!=2019.12.17\r\n",
      "  Downloading regex-2022.7.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 765 kB 82 kB/s eta 0:00:011\r\n",
      "\u001B[?25hCollecting pyyaml>=5.1\r\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 661 kB 693 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/humza/miniconda3/lib/python3.9/site-packages (from transformers) (4.63.0)\r\n",
      "Collecting filelock\r\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\r\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\r\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 101 kB 769 kB/s ta 0:00:011\r\n",
      "\u001B[?25hCollecting numpy>=1.17\r\n",
      "  Downloading numpy-1.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 17.1 MB 1.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting typing-extensions>=3.7.4.3\r\n",
      "  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/humza/miniconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.8)\r\n",
      "Installing collected packages: typing-extensions, pyyaml, filelock, tokenizers, regex, numpy, huggingface-hub, transformers\r\n",
      "Successfully installed filelock-3.8.0 huggingface-hub-0.8.1 numpy-1.23.1 pyyaml-6.0 regex-2022.7.25 tokenizers-0.12.1 transformers-4.21.1 typing-extensions-4.3.0\r\n",
      "Collecting git+https://github.com/huggingface/optimum.git\r\n",
      "  Cloning https://github.com/huggingface/optimum.git to /tmp/pip-req-build-8esgx8d0\r\n",
      "  Running command git clone -q https://github.com/huggingface/optimum.git /tmp/pip-req-build-8esgx8d0\r\n",
      "  Resolved https://github.com/huggingface/optimum.git to commit 69f2883d945ccc669bdd0c78e0ba8fe4265c6792\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h    Preparing wheel metadata ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy in /home/humza/miniconda3/lib/python3.9/site-packages (from optimum==1.4.0.dev0) (1.23.1)\r\n",
      "Requirement already satisfied: packaging in /home/humza/miniconda3/lib/python3.9/site-packages (from optimum==1.4.0.dev0) (21.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from optimum==1.4.0.dev0) (0.8.1)\r\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.20.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from optimum==1.4.0.dev0) (4.21.1)\r\n",
      "Collecting torch>=1.9\r\n",
      "  Downloading torch-1.12.1-cp39-cp39-manylinux1_x86_64.whl (776.4 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 776.4 MB 20 kB/s  eta 0:00:014   |████                            | 98.5 MB 1.3 MB/s eta 0:09:01     |███████████████▊                | 380.2 MB 882 kB/s eta 0:07:29     |████████████████▎               | 395.9 MB 884 kB/s eta 0:07:11\r\n",
      "\u001B[?25hCollecting sympy\r\n",
      "  Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 6.4 MB 674 kB/s eta 0:00:01     |█████████████████████████████▌  | 5.9 MB 674 kB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting coloredlogs\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 46 kB 1.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (4.3.0)\r\n",
      "Requirement already satisfied: requests in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (2.27.1)\r\n",
      "Requirement already satisfied: tqdm in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (4.63.0)\r\n",
      "Requirement already satisfied: filelock in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (3.8.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/humza/miniconda3/lib/python3.9/site-packages (from packaging->optimum==1.4.0.dev0) (3.0.9)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/humza/miniconda3/lib/python3.9/site-packages (from transformers[sentencepiece]>=4.20.1->optimum==1.4.0.dev0) (2022.7.25)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from transformers[sentencepiece]>=4.20.1->optimum==1.4.0.dev0) (0.12.1)\r\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\r\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 1.3 MB 924 kB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting protobuf<=3.20.1\r\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 1.0 MB 1.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting humanfriendly>=9.1\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 86 kB 1.9 MB/s eta 0:00:011\r\n",
      "\u001B[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (1.26.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.4.0.dev0) (2.0.4)\r\n",
      "Collecting mpmath>=0.19\r\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 532 kB 1.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hBuilding wheels for collected packages: optimum\r\n",
      "  Building wheel for optimum (PEP 517) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for optimum: filename=optimum-1.4.0.dev0-py3-none-any.whl size=125728 sha256=d0b42c92fb37788f90c3a8cdbe427a1b11a321249e1ba4a1f17a29ea1fd33898\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mdq5unvf/wheels/a2/2f/00/2a677c0c9bccd235c10e1cda441d7bb0d48ab534cff60239e1\r\n",
      "Successfully built optimum\r\n",
      "Installing collected packages: sentencepiece, protobuf, mpmath, humanfriendly, torch, sympy, coloredlogs, optimum\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 mpmath-1.2.1 optimum-1.4.0.dev0 protobuf-3.20.1 sentencepiece-0.1.97 sympy-1.10.1 torch-1.12.1\r\n",
      "Collecting onnxruntime\r\n",
      "  Downloading onnxruntime-1.12.1-cp39-cp39-manylinux_2_27_x86_64.whl (4.9 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 4.9 MB 919 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: packaging in /home/humza/miniconda3/lib/python3.9/site-packages (from onnxruntime) (21.3)\r\n",
      "Requirement already satisfied: protobuf in /home/humza/miniconda3/lib/python3.9/site-packages (from onnxruntime) (3.20.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from onnxruntime) (1.23.1)\r\n",
      "Collecting flatbuffers\r\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: sympy in /home/humza/miniconda3/lib/python3.9/site-packages (from onnxruntime) (1.10.1)\r\n",
      "Requirement already satisfied: coloredlogs in /home/humza/miniconda3/lib/python3.9/site-packages (from onnxruntime) (15.0.1)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from coloredlogs->onnxruntime) (10.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/humza/miniconda3/lib/python3.9/site-packages (from packaging->onnxruntime) (3.0.9)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/humza/miniconda3/lib/python3.9/site-packages (from sympy->onnxruntime) (1.2.1)\r\n",
      "Installing collected packages: flatbuffers, onnxruntime\r\n",
      "Successfully installed flatbuffers-2.0 onnxruntime-1.12.1\r\n",
      "Collecting onnx\r\n",
      "  Downloading onnx-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 13.1 MB 1.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /home/humza/miniconda3/lib/python3.9/site-packages (from onnx) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from onnx) (4.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/humza/miniconda3/lib/python3.9/site-packages (from onnx) (1.23.1)\r\n",
      "Installing collected packages: onnx\r\n",
      "Successfully installed onnx-1.12.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!python -m pip install git+https://github.com/huggingface/optimum.git\n",
    "!pip install onnxruntime\n",
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmarking"
   ],
   "metadata": {
    "id": "iZmyw099C7lz",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction"
   ],
   "metadata": {
    "id": "0rrypFygicbb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base Model Load\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "raC34vqLC_cv",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"microsoft/codebert-base\"\n",
    "task = \"feature-extraction\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2 \n",
    "feature_extractor = pipeline(task, model=model_id, tokenizer=tokenizer, handle_impossible_answer=True)\n"
   ],
   "metadata": {
    "id": "v9BQSmhwC-TD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/humza/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<00:00, 13.1kB/s]\n",
      "Downloading config.json: 100%|██████████| 498/498 [00:00<00:00, 282kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 878k/878k [00:14<00:00, 62.7kB/s] \n",
      "Downloading merges.txt: 100%|██████████| 446k/446k [00:04<00:00, 111kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|██████████| 150/150 [00:00<00:00, 79.2kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 476M/476M [09:19<00:00, 892kB/s]    \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "prediction = feature_extractor(\"What's my name?\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVXsiNeXETwS",
    "outputId": "6fc410bb-54d1-43e6-f01b-a2589ebd0b11",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 408 ms, sys: 51.8 ms, total: 460 ms\n",
      "Wall time: 476 ms\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimized Model"
   ],
   "metadata": {
    "id": "ONWx3fMnEceB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "model_id = \"microsoft/codebert-base\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"feature-extraction\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2\n",
    "optimum_feature_extractor = pipeline(task, model=model, tokenizer=tokenizer, handle_impossible_answer=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a0ef06c15b14496da01b239592f10bef",
      "d4c06e8ce9624b32b56797d7f72fa68d",
      "cdfc81d124354bf1a329f22c595852d9",
      "d9c757df508944daabb0d13d50e1496b",
      "5959295bcf9843d9b1a1203bb071f02c",
      "3d2a32dc0ef64f8b9d362978917f31b8",
      "2fc20f5e83154a2b9f0c4dd7d83140c4",
      "41a11af023264d809af04468e11a3e35",
      "04ec75fe4aca42a3b70bfa34203785fe",
      "e464fb4ba31849c6a8156dc28a95b711",
      "00820d3385294adfbfc1d891be3b0ca8"
     ]
    },
    "id": "Q4uWoNZtEYxj",
    "outputId": "cdc20373-ffa0-4cfa-c534-fb134c5ca973",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 498/498 [00:00<00:00, 320kB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\r\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 365 kB 482 kB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting multiprocess\r\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 132 kB 7.0 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: packaging in /home/humza/miniconda3/lib/python3.9/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from datasets) (2.27.1)\r\n",
      "Requirement already satisfied: pandas in /home/humza/miniconda3/lib/python3.9/site-packages (from datasets) (1.4.3)\r\n",
      "Collecting fsspec[http]>=2021.11.1\r\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 141 kB 7.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /home/humza/miniconda3/lib/python3.9/site-packages (from datasets) (1.23.1)\r\n",
      "Collecting xxhash\r\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 211 kB 7.1 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting pyarrow>=6.0.0\r\n",
      "  Downloading pyarrow-9.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 35.3 MB 295 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.62.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from datasets) (4.63.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from datasets) (0.7.0)\r\n",
      "Collecting responses<0.19\r\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\r\n",
      "Requirement already satisfied: dill<0.3.6 in /home/humza/miniconda3/lib/python3.9/site-packages (from datasets) (0.3.5.1)\r\n",
      "Collecting aiohttp\r\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 1.2 MB 253 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\r\n",
      "Requirement already satisfied: filelock in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/humza/miniconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.8)\r\n",
      "Collecting aiosignal>=1.1.2\r\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\r\n",
      "Collecting frozenlist>=1.1.1\r\n",
      "  Downloading frozenlist-1.3.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 158 kB 286 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: attrs>=17.3.0 in /home/humza/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\r\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\r\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\r\n",
      "Collecting multidict<7.0,>=4.5\r\n",
      "  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 114 kB 154 kB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting yarl<2.0,>=1.0\r\n",
      "  Downloading yarl-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 264 kB 151 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/humza/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/humza/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\r\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, pyarrow, multiprocess, datasets\r\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.4.0 frozenlist-1.3.1 fsspec-2022.7.1 multidict-6.0.2 multiprocess-0.70.13 pyarrow-9.0.0 responses-0.18.0 xxhash-3.0.0 yarl-1.8.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task: feature-extraction. Possible values are [<class 'transformers.models.auto.modeling_auto.AutoModel'>, <class 'transformers.models.auto.modeling_auto.AutoModelForMaskedLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.auto.modeling_auto.AutoModelForMultipleChoice'>, <class 'transformers.models.auto.modeling_auto.AutoModelForQuestionAnswering'>, <class 'transformers.models.auto.modeling_auto.AutoModelForImageClassification'>, <class 'transformers.models.auto.modeling_auto.AutoModelForMaskedImageModeling'>]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m task \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature-extraction\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# create ORTOptimizer and define optimization configuration\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m \u001B[43mORTOptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m optimization_config \u001B[38;5;241m=\u001B[39m OptimizationConfig(optimization_level\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m99\u001B[39m) \u001B[38;5;66;03m# enable all optimizations\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# apply the optimization configuration to the model\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/optimum/onnxruntime/optimization.py:60\u001B[0m, in \u001B[0;36mORTOptimizer.from_pretrained\u001B[0;34m(model_name_or_path, feature, opset)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03mInstantiate a `ORTOptimizer` from a pretrained pytorch model and preprocessor.\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    An instance of `ORTOptimizer`.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     59\u001B[0m preprocessor \u001B[38;5;241m=\u001B[39m get_preprocessor(model_name_or_path)\n\u001B[0;32m---> 60\u001B[0m model_class \u001B[38;5;241m=\u001B[39m \u001B[43mFeaturesManager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_model_class_for_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeature\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m model \u001B[38;5;241m=\u001B[39m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name_or_path)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ORTOptimizer(preprocessor, model, feature, opset)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/onnx/features.py:468\u001B[0m, in \u001B[0;36mFeaturesManager.get_model_class_for_feature\u001B[0;34m(feature, framework)\u001B[0m\n\u001B[1;32m    466\u001B[0m     task_to_automodel \u001B[38;5;241m=\u001B[39m FeaturesManager\u001B[38;5;241m.\u001B[39m_TASKS_TO_TF_AUTOMODELS\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m task \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m task_to_automodel:\n\u001B[0;32m--> 468\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m    469\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown task: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfeature\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Possible values are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(FeaturesManager\u001B[38;5;241m.\u001B[39m_TASKS_TO_AUTOMODELS\u001B[38;5;241m.\u001B[39mvalues())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    470\u001B[0m     )\n\u001B[1;32m    471\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m task_to_automodel[task]\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Unknown task: feature-extraction. Possible values are [<class 'transformers.models.auto.modeling_auto.AutoModel'>, <class 'transformers.models.auto.modeling_auto.AutoModelForMaskedLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.auto.modeling_auto.AutoModelForMultipleChoice'>, <class 'transformers.models.auto.modeling_auto.AutoModelForQuestionAnswering'>, <class 'transformers.models.auto.modeling_auto.AutoModelForImageClassification'>, <class 'transformers.models.auto.modeling_auto.AutoModelForMaskedImageModeling'>]\""
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "model_id = \"microsoft/codebert-base\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "task = \"feature-extraction\"\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model_id, feature=task)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_optimized_model_output_path=onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "prediction = optimum_feature_extractor(\"What's my name?\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFDPyxIUErpn",
    "outputId": "e96bfc46-3d23-4892-9f25-4d98c8f8e613",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 131 ms, sys: 2.26 ms, total: 133 ms\n",
      "Wall time: 120 ms\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question Answering"
   ],
   "metadata": {
    "id": "kpc7mDGUixqu",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base Model"
   ],
   "metadata": {
    "id": "kBmUbLOqi249",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "task = \"question-answering\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2 \n",
    "qa_model = pipeline(task, model=model_id, tokenizer=tokenizer, handle_impossible_answer=True)"
   ],
   "metadata": {
    "id": "p9teFEo8jEWI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 79.0/79.0 [00:00<00:00, 33.7kB/s]\n",
      "Downloading config.json: 100%|██████████| 571/571 [00:00<00:00, 268kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 878k/878k [00:05<00:00, 166kB/s]  \n",
      "Downloading merges.txt: 100%|██████████| 446k/446k [00:10<00:00, 42.4kB/s] \n",
      "Downloading special_tokens_map.json: 100%|██████████| 772/772 [00:00<00:00, 293kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 473M/473M [06:57<00:00, 1.19MB/s] \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "prediction = qa_model(question=\"what is my name?\", context=\"My name is Humza and I'm 5 years old\")\n",
    "print(prediction)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHeXzSGHjEgX",
    "outputId": "757ada32-1e76-4fd9-d6fe-f869837bd135",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6795036196708679, 'start': 11, 'end': 16, 'answer': 'Humza'}\n",
      "CPU times: user 2.66 s, sys: 0 ns, total: 2.66 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimized Model"
   ],
   "metadata": {
    "id": "RTmS1nUsi59B",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx_qa\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "94ccc999334044cdbd502d5c5cf021bf",
      "18ba8f1c9abd448b99935ce008c00c23",
      "78b99f15dbce47cb9aaf92c6b891b4d7",
      "c8d23799f62b4cb2915c94b86332bf68",
      "a9a885bc5b7e49778fc19675ed71cf58",
      "c10a910ceb074e24aff32369f64f5155",
      "9a8544fb1e124d0f8a66cb98cc174a15",
      "5f2dad47f9da481cb185feab60ab4e06",
      "51fdd1c8de3f491ea8f86cc86665ff5d",
      "4854182869074745873f5d7f4a0d961c",
      "8cde3c6bd0f44f6bbc1ec247925af7e2"
     ]
    },
    "id": "_a9vdsBOq_dC",
    "outputId": "32160fba-cc9d-4bb4-abe3-638f80a0bdc2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 32.5kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "('onnx_qa/tokenizer_config.json',\n 'onnx_qa/special_tokens_map.json',\n 'onnx_qa/vocab.json',\n 'onnx_qa/merges.txt',\n 'onnx_qa/added_tokens.json',\n 'onnx_qa/tokenizer.json')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model_id, feature=task)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_optimized_model_output_path=onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bGiO42yrS2g",
    "outputId": "74497b0d-ccaa-4673-dce9-d97a5eee619c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import optimum.onnxruntime.optimization because of the following error (look up to see its traceback):\nNo module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1002\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1002\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/importlib/__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1030\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:986\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:680\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:850\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:228\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/optimum/onnxruntime/optimization.py:29\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01monnxruntime\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_fusion_statistics, optimize_model\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfiguration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OptimizationConfig\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ORTConfigManager\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/optimum/onnxruntime/configuration.py:20\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dict, List, Optional, Tuple, Union\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Version, parse\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'datasets'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptimum\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnxruntime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ORTOptimizer\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptimum\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnxruntime\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfiguration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OptimizationConfig\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# create ORTOptimizer and define optimization configuration\u001B[39;00m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1055\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[0;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:992\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    990\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[1;32m    991\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m--> 992\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    993\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[1;32m    994\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1004\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1003\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1004\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1005\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1006\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1007\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import optimum.onnxruntime.optimization because of the following error (look up to see its traceback):\nNo module named 'datasets'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "# load quantized model\n",
    "opt_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model-optimized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "optimum_qa_model = pipeline(task, model=opt_model, tokenizer=tokenizer, handle_impossible_answer=True)"
   ],
   "metadata": {
    "id": "A6nAxzm3rXe4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "prediction = optimum_qa_model(question=\"what is my name?\", context=\"My name is Humza and I'm 5 years old\")\n",
    "print(prediction)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9C0_GQYdjFhk",
    "outputId": "574a1ece-e289-428b-aced-72837fa4acbf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quantized Model"
   ],
   "metadata": {
    "id": "C3-SnFQBi_MS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id, feature=task)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model-optimized.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized.onnx\",\n",
    "    quantization_config=qconfig,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "On_rSehljGTs",
    "outputId": "d91d9b78-2979-4a57-862a-c9b7f59cbb01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load quantized model\n",
    "qt_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model-quantized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "quantized_qa_model = pipeline(task, model=qt_model, tokenizer=tokenizer, handle_impossible_answer=True)"
   ],
   "metadata": {
    "id": "_PB9dydBSRsQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "prediction = quantized_qa_model(question=\"what is my name?\", context=\"My name is Humza and I'm 5 years old\")\n",
    "print(prediction)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnFlFRnOsbwc",
    "outputId": "5fe55f51-8037-470f-9c9e-88f38b282f58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "P_qgfH2EsjMf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}